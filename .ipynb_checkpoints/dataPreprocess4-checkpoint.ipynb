{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing \n",
    "#### ④ Make embedding Features\n",
    "\n",
    "keras embedding layer를 통해 도서의 정보를 활용할 수 있도록 각각의 feature에 대한 embedding을 진행한다. 자연어로 이루어진 특성들은 BERT 모델로, 도서의 표지는 EfficientNet B0를 통해 일종의 transfer learning를 진행해서 효과적으로 특성을 추출한다. 도서의 페이지와 출판 연도와 같은 수치형 특성들은 제곱근과 제곱을 통해 특성을 부풀려주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import math \n",
    "from keras import models\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = pd.read_csv('book_final.csv')\n",
    "student = pd.read_csv('student_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_using_BERT(model, data_column):\n",
    "    embedding_feature = model.encode(data_column)\n",
    "    print(\">>> CHECK SHAPE: \", embedding_feature.shape)\n",
    "    \n",
    "    return embedding_feature\n",
    "\n",
    "def sqrt_pow(row):\n",
    "    row['page_sqrt'] = math.sqrt(float(row['page']))\n",
    "    row['page_pow'] = math.pow(row['page'], 2)\n",
    "    \n",
    "    row['year_sqrt'] = math.sqrt(float(row['year']))\n",
    "    row['year_pow'] = math.pow(row['page'], 2)\n",
    "    \n",
    "    return row\n",
    "\n",
    "def preprocessed(book_ids, img_path):\n",
    "    new_images = []\n",
    "    no_image_ids = []\n",
    "    \n",
    "    for i in tq.tqdm(range(len(book_ids))):\n",
    "        book_file = img_path + book_ids[i] + '.jpg'\n",
    "        \n",
    "        try:\n",
    "            image = load_img(book_file, target_size = (224, 224))\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image = preprocess_input(image)\n",
    "            new_images.append(image)\n",
    "            \n",
    "        except:\n",
    "            no_image_ids.append(book_file)\n",
    "            print(book_file, end = ' ')\n",
    "        \n",
    "    return new_images, no_image_ids\n",
    "\n",
    "def get_features(extractor, image_list):\n",
    "    img_features = extractor.predict(image_list[0], verbose=0)\n",
    "    \n",
    "    for i in tq.tqdm(range(1, len(image_list))):\n",
    "        features = extractor.predict(image_list[i], verbose=0)\n",
    "        img_features = np.append(img_features, features, axis = 0)\n",
    "    \n",
    "    return img_features\n",
    "\n",
    "\n",
    "def feature_extract_pipeline(extractor, img_path, book_ids):\n",
    "    preprocessed_images, no_images = preprocessed(book_ids, img_path)\n",
    "    image_features = get_features(extractor, preprocessed_images)\n",
    "    print(\"CHECK SHAPE: \", image_features.shape)\n",
    "    \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    bert_model = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')\n",
    "    \n",
    "    # book data embedding\n",
    "    title_embedd = word_embedding_using_BERT(bert_model, book['title'])\n",
    "    genre_embedd = word_embedding_using_BERT(bert_model, book['genre'])\n",
    "    page_year = book[['page', 'pub_year']]\n",
    "    page_year = page_year.progress_apply(lambda x : sqrt_pow(x), axis = 1)\n",
    "    \n",
    "    # book data embedding - book cover\n",
    "    base_model = EfficientNetB0(weights='imagenet')\n",
    "    model_eff = models.Model(inputs = base_model.input, outputs = base_model.get_layer('avg_pool').output)\n",
    "\n",
    "    book_ids = book['book_id']\n",
    "    image_path = '../image/'\n",
    "\n",
    "    image_features = feature_extract_pipeline(model_eff, image_path, book_ids)\n",
    "    \n",
    "    ## student college embedding\n",
    "    major_list = sorted(student['college'].unique())\n",
    "    major_embedd = word_embedding_using_BERT(bert_model, major_list)\n",
    "    \n",
    "    # np.save('page_year_embed.npy', page_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
